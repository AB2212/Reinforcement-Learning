{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Setting up environment\n",
    "env=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ###Network Architecture\n",
    "        self.Linear1=nn.Linear(4,64)\n",
    "#         self.Linear2=nn.Linear(256,32)\n",
    "        self.output_layer= nn.Linear(64,2)\n",
    "        self.Dropout=nn.Dropout(p=0.1)\n",
    "        \n",
    "        ###Saving history\n",
    "        self.saved_log_probs=[]\n",
    "        self.rewards=[]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        ###Network Architecture flow\n",
    "        x=F.relu(self.Linear1(x))\n",
    "#         x=self.Dropout(x)\n",
    "#         x=F.relu(self.Linear2(x))\n",
    "        x=self.Dropout(x)\n",
    "        x=self.output_layer(x)\n",
    "        \n",
    "        return F.softmax(x,dim=1)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Initializing the network\n",
    "policy=Policy()\n",
    "optimizer=optim.Adam(policy.parameters(),lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \n",
    "    ###Converting to tensor and adding a dimension\n",
    "    state=torch.from_numpy(state).float().unsqueeze(0)\n",
    "    \n",
    "    ###Passing state through network to get probability\n",
    "    probs=policy(state)\n",
    "    \n",
    "    ###Sampling action\n",
    "    m=Categorical(probs)\n",
    "    action=m.sample()\n",
    "    \n",
    "    ###Saving log of probability for gradient calculation\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    \n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finish_episode(gamma=0.8):\n",
    "    \n",
    "    R=0\n",
    "    rewards=policy.rewards[::-1]\n",
    "    returns=[]\n",
    "    policy_loss=[]\n",
    "    for r in rewards:\n",
    "        R=r+gamma*R\n",
    "        returns.insert(0,R)\n",
    "        \n",
    "    returns=torch.Tensor(returns)\n",
    "    \n",
    "    returns=(returns-returns.mean())/(returns.std()+eps)\n",
    "    \n",
    "    loss=0\n",
    "    \n",
    "    for log_prob,R in zip(policy.saved_log_probs,returns):\n",
    "        policy_loss.append(-log_prob*R)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss=torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    T=200\n",
    "    log_interval=100\n",
    "    running_reward=1\n",
    "    for i_episode in count(1):\n",
    "        state=env.reset()\n",
    "        ep_reward=0\n",
    "        \n",
    "        for t in range(T):\n",
    "            ###Selecting action\n",
    "            action=select_action(state)\n",
    "            state,reward,done,_=env.step(action)\n",
    "            \n",
    "            env.render()\n",
    "            #Saving Rewards\n",
    "            policy.rewards.append(reward)\n",
    "            #Updating episode reward\n",
    "            ep_reward+=reward\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        finish_episode()\n",
    "        \n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "            \n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast reward: 42.00\tAverage reward: 132.09\n",
      "Episode 200\tLast reward: 200.00\tAverage reward: 186.63\n",
      "Solved! Running reward is now 195.207966266448 and the last episode runs to 199 time steps!\n"
     ]
    }
   ],
   "source": [
    "#main()\n",
    "\n",
    "#torch.save(policy, \"./models/policy_June9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/policy_June9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(model):\n",
    "    \n",
    "    env=gym.make('CartPole-v0')\n",
    "    T=200\n",
    "    for i_episode in range(10):\n",
    "        state=env.reset()\n",
    "        ep_reward=0\n",
    "        \n",
    "        for t in range(T):\n",
    "            #####Selecting action\n",
    "            ###Converting to tensor and adding a dimension\n",
    "            state=torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "            ###Passing state through network to get probability\n",
    "            probs=model(state)\n",
    "\n",
    "            ###Sampling action\n",
    "            m=Categorical(probs)\n",
    "            action=m.sample().item()\n",
    "            \n",
    "            state,reward,done,_=env.step(action)\n",
    "            \n",
    "            env.render()\n",
    "            #Updating episode reward\n",
    "            ep_reward+=reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "      \n",
    "        print('Episode {}\\tEpisode reward: {:.2f}'.format(\n",
    "              i_episode, ep_reward))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0\tEpisode reward: 200.00\n",
      "Episode 1\tEpisode reward: 200.00\n",
      "Episode 2\tEpisode reward: 200.00\n",
      "Episode 3\tEpisode reward: 200.00\n",
      "Episode 4\tEpisode reward: 200.00\n",
      "Episode 5\tEpisode reward: 200.00\n",
      "Episode 6\tEpisode reward: 200.00\n",
      "Episode 7\tEpisode reward: 200.00\n",
      "Episode 8\tEpisode reward: 200.00\n",
      "Episode 9\tEpisode reward: 200.00\n"
     ]
    }
   ],
   "source": [
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
