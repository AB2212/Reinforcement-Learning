{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Setting up environment\n",
    "env=gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_states=None,\n",
    "                 hidden_states=None,\n",
    "                 num_actions=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ###Network Architecture\n",
    "        \n",
    "        #Shared layer\n",
    "        self.shared_layer =nn.Linear(num_states,hidden_states)\n",
    "        # Actor layer\n",
    "        self.action_layer = nn.Linear(hidden_states,num_actions)\n",
    "        # Critic layer\n",
    "        self.value_layer = nn.Linear(hidden_states, 1)\n",
    "            \n",
    "    \n",
    "        #self.Dropout=nn.Dropout(p=0.1)\n",
    "        \n",
    "        ###Saving history\n",
    "        self.saved_actions=[]\n",
    "        self.rewards=[]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        ###Network Architecture flow\n",
    "        \n",
    "        x = F.relu(self.shared_layer(x))\n",
    "        \n",
    "        critic_val = self.value_layer(x)\n",
    "        \n",
    "        return F.softmax(self.action_layer(x),dim=1), critic_val\n",
    "    \n",
    "    \n",
    "def weight_init(m):\n",
    "    \n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Initializing the network\n",
    "policy=Policy(num_states=8,\n",
    "              hidden_states=256,\n",
    "              num_actions=env.action_space.n)\n",
    "#policy.apply(weight_init)\n",
    "optimizer=optim.Adam(policy.parameters(),lr=3e-3,amsgrad=True)\n",
    "\n",
    "eps = np.finfo(np.float64).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \n",
    "    ###Converting to tensor and adding a dimension\n",
    "    state=torch.from_numpy(state).float().unsqueeze(0)\n",
    "    \n",
    "    ###Passing state through network to get probability\n",
    "    probs, state_value=policy(state)\n",
    "    \n",
    "    ###Sampling action\n",
    "    m=Categorical(probs)\n",
    "    action=m.sample()\n",
    "    \n",
    "    ###Saving log of probability for gradient calculation\n",
    "    policy.saved_actions.append((m.log_prob(action), state_value))\n",
    "    \n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finish_episode(gamma=0.99):\n",
    "    \n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = policy.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([[R]])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_actions[:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #T=200\n",
    "    log_interval=10\n",
    "    running_reward=0\n",
    "    past_reward=0\n",
    "    for i_episode in count(1):\n",
    "        state=env.reset()\n",
    "        ep_reward=0\n",
    "        \n",
    "        while(True):\n",
    "            ###Selecting action\n",
    "            action=select_action(state)\n",
    "            state,reward,done,_=env.step(action)\n",
    "            \n",
    "            #env.render()\n",
    "            #Saving Rewards\n",
    "            policy.rewards.append(reward)\n",
    "            #Updating episode reward\n",
    "            ep_reward+=reward\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        finish_episode()\n",
    "        \n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "            if running_reward>50 and running_reward>past_reward:\n",
    "                past_reward=running_reward\n",
    "                torch.save(policy, \"./models/model_Nov25_ex1_%s\"%i_episode)\n",
    "            \n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, i_episode))\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#main()\n",
    "\n",
    "#torch.save(policy, \"./models/policy_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_Nov25_ex1_1300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(model):\n",
    "    \n",
    "    env=gym.make('LunarLander-v2')\n",
    "    #T=200\n",
    "    for i_episode in range(10):\n",
    "        state=env.reset()\n",
    "        ep_reward=0\n",
    "        \n",
    "        while(True):\n",
    "            #####Selecting action\n",
    "            ###Converting to tensor and adding a dimension\n",
    "            state=torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "            ###Passing state through network to get probability\n",
    "            probs,value=model(state)\n",
    "\n",
    "            ###Sampling action\n",
    "            m=Categorical(probs)\n",
    "            action=m.sample().item()\n",
    "\n",
    "            #action =torch.argmax(probs).item()\n",
    "    \n",
    "#             print(action)\n",
    "\n",
    "            \n",
    "            state,reward,done,_=env.step(action)\n",
    "            \n",
    "            env.render()\n",
    "            #Updating episode reward\n",
    "            ep_reward+=reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "      \n",
    "        print('Episode {}\\tEpisode reward: {:.2f}'.format(\n",
    "              i_episode, ep_reward))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0\tEpisode reward: 202.72\n",
      "Episode 1\tEpisode reward: 198.56\n",
      "Episode 2\tEpisode reward: 214.19\n",
      "Episode 3\tEpisode reward: 198.07\n",
      "Episode 4\tEpisode reward: 156.82\n",
      "Episode 5\tEpisode reward: 199.56\n",
      "Episode 6\tEpisode reward: 119.56\n",
      "Episode 7\tEpisode reward: 168.52\n",
      "Episode 8\tEpisode reward: 160.33\n"
     ]
    }
   ],
   "source": [
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
